<html>

<head>
	<title> Audio Visualization </title>
</head>

<body>
	<h1> Visualize Audio from Microphone Input </h1>
	<p> Web audio API example: takes audio input from microphone visualizes the waveform and spectogram in real-time. </p>

	<p> <canvas id="wave_view" style="background: white;"> </canvas> </p>
	<p> <canvas id="spec_view" style="background: white;"> </canvas> </p>

	<script>
	window.AudioContext = window.AudioContext || window.webkitAudioContext;

	var context = new AudioContext();

	var myAudioBuffer = null;
	var analyser = context.createAnalyser();
	analyser.fftSize = 2048;
	analyser.smoothingTimeConstant = 0;

	var WIDTH = 1024;
	var HEIGHT = 256;

	var wave_view = document.getElementById("wave_view");
	wave_view.width = WIDTH;
	wave_view.height = HEIGHT;

	var spec_view = document.getElementById("spec_view");
	spec_view.width = WIDTH;
	spec_view.height = HEIGHT;
	
	function draw_wave() {
		// 2d canvas context
		var drawContext = wave_view.getContext('2d');

		// fill rectangular
		drawContext.fillStyle = 'rgb(200, 200, 200)';
		drawContext.fillRect(0, 0, WIDTH, HEIGHT);

		// drawing line setting
		drawContext.lineWidth = 2;
		drawContext.strokeStyle = 'rgb(0, 0, 0)';
		drawContext.beginPath();

		// get samples
		var dataArray = new Uint8Array(analyser.frequencyBinCount);
		analyser.getByteTimeDomainData(dataArray);

		var sliceWidth = WIDTH * 1.0 / dataArray.length;
		var x = 0;

		for (var i = 0; i < dataArray.length; i++) {
			var v = dataArray[i] / 256.0;
			var y = v * HEIGHT;

			if ( i === 0 ) {
				drawContext.moveTo(x, y);
			} else {
				drawContext.lineTo(x, y);
			}

			x += sliceWidth;
		}


		// last touch
		drawContext.lineTo(wave_view.width, wave_view.height / 2);
		drawContext.stroke();

		// queue for next callback
		window.requestAnimationFrame(draw_wave);
	}

	function draw_spec() {
		// 2d canvas context
		var drawContext = spec_view.getContext('2d');

		// fill rectangular
		drawContext.fillStyle = 'rgb(100, 100, 100)';
		drawContext.fillRect(0, 0, WIDTH, HEIGHT);

		// drawing line setting
		drawContext.lineWidth = 2;
		drawContext.strokeStyle = 'rgb(0, 0, 0)';
		drawContext.beginPath();

		// get samples
		var dataArray = new Uint8Array(analyser.frequencyBinCount);
		analyser.getByteFrequencyData(dataArray);

		var sliceWidth = WIDTH * 1.0 / dataArray.length;
		var x = 0;

		for (var i = 0; i < dataArray.length; i++) {
			var v = dataArray[i] / 128.0;
			var y = HEIGHT - v * HEIGHT / 2;

			if ( i === 0 ) {
				drawContext.moveTo(x, y);
			} else {
				drawContext.lineTo(x, y);
			}

			x += sliceWidth;
		}

		// last touch
		drawContext.lineTo(draw_spec.width, draw_spec.height / 2);
		drawContext.stroke();

		// queue for next callback
		window.requestAnimationFrame(draw_spec);
	}




	if (!navigator.getUserMedia)
		navigator.getUserMedia = (navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia);

	if (!navigator.getUserMedia)
		alert("Error: getUserMedia not supported!");

	// get audio input streaming
	navigator.getUserMedia({audio: true}, onStream, onStreamError)

	// success Callback
	function onStream(stream) {
		var input = context.createMediaStreamSource(stream);

		// connect graph
		input.connect(analyser);

		// visualize audio
		draw_wave();
		draw_spec();
	}

	// error Callback
	function onStreamError(error) {
		console.error('Error getting microphene', error);
	}


	</script>


</body>


</html>
